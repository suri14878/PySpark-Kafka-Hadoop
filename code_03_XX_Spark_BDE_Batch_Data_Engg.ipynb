{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61fbc6d-310d-450a-8d68-9802796cffce",
   "metadata": {},
   "source": [
    "# Stock Aggregation Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f862f49-381e-4372-baec-d86330d3eb25",
   "metadata": {},
   "source": [
    "### 03.03.Setup the local database\n",
    "\n",
    "Create sample data in the local warehouse database. This will be created for 3 warehouses, namely NewYork, Los Angeles and London. For convenience sake, data for all 3 warehouses will be stored in the same database. In a realworld situation, each warehouse will have its own database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15391446-a032-43fc-a48f-9d6b712b59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a database connection & cursor\n",
    "\n",
    "import mariadb\n",
    "\n",
    "#Connect to warehouse_stock database\n",
    "local_db_conn = mariadb.connect(\n",
    "                user=\"spark\",\n",
    "                password=\"spark\",\n",
    "                host=\"127.0.0.1\",\n",
    "                port=3306,\n",
    "                database=\"warehouse_stock\",\n",
    "                autocommit=True\n",
    "            )\n",
    "\n",
    "local_cursor = local_db_conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f45dd3-ebc9-4274-92a5-55894fe9272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete existing data if needed. Execute code if needed to reinitialize database\n",
    "local_cursor.execute(\"DELETE FROM warehouse_stock.item_stock\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35837101-58f8-46ff-ba92-5979beee1bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample data for warehouse :  NewYork\n",
      "Generating data for date :  2025-12-29\n",
      "Generating data for date :  2025-12-30\n",
      "Generating data for date :  2025-12-31\n",
      "Generating sample data for warehouse :  LosAngeles\n",
      "Generating data for date :  2025-12-29\n",
      "Generating data for date :  2025-12-30\n",
      "Generating data for date :  2025-12-31\n",
      "Generating sample data for warehouse :  London\n",
      "Generating data for date :  2025-12-29\n",
      "Generating data for date :  2025-12-30\n",
      "Generating data for date :  2025-12-31\n",
      "\n",
      "Records created:\n",
      "---------------------------\n",
      "London  :  30\n",
      "LosAngeles  :  30\n",
      "NewYork  :  30\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "\n",
    "def generate_data(generate_dates, warehouse_id):\n",
    "    print(\"Generating sample data for warehouse : \", warehouse_id)\n",
    "\n",
    "    #Create sample list of products and their unit values\n",
    "    item_list = {\n",
    "        \"Tape Dispenser\" : 5.99,\n",
    "        \"Pencil Sharpener\" : 10.00,\n",
    "        \"Labeling Machine\" : 25.00,\n",
    "        \"Calculator\" : 14.99,\n",
    "        \"Scissors\" : 7.99,\n",
    "        \"Sticky Notes\" : 2.00,\n",
    "        \"Notebook\" : 2.50,\n",
    "        \"Clipboard\" : 12.00,\n",
    "        \"Folder\" : 1.00,\n",
    "        \"Pencil Box\" : 2.99\n",
    "    }\n",
    "\n",
    "\n",
    "    #Loop for each date\n",
    "    for gen_date in generate_dates:\n",
    "        print(\"Generating data for date : \", gen_date)\n",
    "\n",
    "        for item, unit_value in item_list.items():\n",
    "            #Generate random values for opening stock, receipts and issues\n",
    "            opening_stock = random.randint(1,100)\n",
    "            receipts=random.randint(1,50)\n",
    "            issues=random.randint(1, opening_stock+receipts)\n",
    "\n",
    "            insert_sql = f\"\"\"\n",
    "                INSERT INTO `warehouse_stock`.`item_stock` \n",
    "                (`STOCK_DATE`, `WAREHOUSE_ID`,`ITEM_NAME`,\n",
    "                    `OPENING_STOCK`,`RECEIPTS`,`ISSUES`,`UNIT_VALUE` )\n",
    "                VALUES ( '{gen_date}','{warehouse_id}','{item}',\n",
    "                    {opening_stock},{receipts},{issues},{unit_value} )\n",
    "            \"\"\"\n",
    "            local_cursor.execute(insert_sql)\n",
    "\n",
    "\n",
    "#Generate last 3 dates to push stock data for\n",
    "generate_dates = [\n",
    "    (datetime.datetime.today()-datetime.timedelta(2)).strftime(\"%Y-%m-%d\"),\n",
    "    (datetime.datetime.today()-datetime.timedelta(1)).strftime(\"%Y-%m-%d\"),\n",
    "    datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    ]\n",
    "    \n",
    "#Generate for 3 warehouses\n",
    "generate_data(generate_dates,\"NewYork\")\n",
    "generate_data(generate_dates,\"LosAngeles\")\n",
    "generate_data(generate_dates,\"London\")\n",
    "\n",
    "#Get count of records\n",
    "local_cursor.execute(\"\"\"SELECT `WAREHOUSE_ID`, count(*) AS RECS\n",
    "                        FROM `warehouse_stock`.`item_stock`\n",
    "                        GROUP BY `WAREHOUSE_ID`\"\"\")\n",
    "\n",
    "print(\"\\nRecords created:\\n---------------------------\")\n",
    "for warehouse_id, recs in local_cursor:\n",
    "    print( warehouse_id, \" : \", recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d0237-9304-48a4-8b2c-ef6f603f4cba",
   "metadata": {},
   "source": [
    "### 03.04. Upload stock to the central store\n",
    "\n",
    "Each warehouse uploads to a central store, mostly an S3 folder or a HDFS folder. We simulate this folder with a local directory. Data is stored as distributed files, partitioned by date and warehouse ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765462b5-94a0-485f-8302-8724970a3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#We will reuse the local_db_conn already created. In real implementation, \n",
    "#this will be a separate script, so database connected need to be created.\n",
    "\n",
    "#create spark session for Windows\n",
    "local_spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"StockUploaderJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
    "            .config(\"spark.default.parallelism\", 2)\\\n",
    "            .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)\\\n",
    "            .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\",\"2\")\\\n",
    "            .config(\"spark.jars\", \"jars/mysql-connector-j-8.4.0.jar\") \\\n",
    "            .config(\"spark.driver.extraClassPath\",\"jars/mysql-connector-j-8.4.0.jar\") \\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "#Set logging level to warn\n",
    "local_spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7a0d00-490c-4833-9518-ced965b59d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading stock for dates 2025-12-29 to 2025-12-31 for NewYork\n",
      "Query bounds are :  1 30\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "| ID|STOCK_DATE|WAREHOUSE_ID|       ITEM_NAME|OPENING_STOCK|RECEIPTS|ISSUES|UNIT_VALUE|\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "|  1|2025-12-29|     NewYork|  Tape Dispenser|            6|      42|    29|      5.99|\n",
      "|  2|2025-12-29|     NewYork|Pencil Sharpener|           34|      15|    21|     10.00|\n",
      "|  3|2025-12-29|     NewYork|Labeling Machine|           94|       4|    14|     25.00|\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Uploading stock for dates 2025-12-29 to 2025-12-31 for LosAngeles\n",
      "Query bounds are :  31 60\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "| ID|STOCK_DATE|WAREHOUSE_ID|       ITEM_NAME|OPENING_STOCK|RECEIPTS|ISSUES|UNIT_VALUE|\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "| 31|2025-12-29|  LosAngeles|  Tape Dispenser|           40|      43|    83|      5.99|\n",
      "| 32|2025-12-29|  LosAngeles|Pencil Sharpener|           59|      25|    72|     10.00|\n",
      "| 33|2025-12-29|  LosAngeles|Labeling Machine|           81|      33|    84|     25.00|\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Uploading stock for dates 2025-12-29 to 2025-12-31 for London\n",
      "Query bounds are :  61 90\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "| ID|STOCK_DATE|WAREHOUSE_ID|       ITEM_NAME|OPENING_STOCK|RECEIPTS|ISSUES|UNIT_VALUE|\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "| 61|2025-12-29|      London|  Tape Dispenser|           50|      29|    73|      5.99|\n",
      "| 62|2025-12-29|      London|Pencil Sharpener|           54|      38|    30|     10.00|\n",
      "| 63|2025-12-29|      London|Labeling Machine|           84|      24|    23|     25.00|\n",
      "+---+----------+------------+----------------+-------------+--------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def upload_stock(start_date,end_date,warehouse_id):\n",
    "    print(f\"\"\"Uploading stock for dates {start_date} to {end_date} for {warehouse_id}\"\"\")\n",
    "\n",
    "    #find min and max bounds for the parallel DB query\n",
    "    local_cursor.execute(f\"\"\"\n",
    "        SELECT min(`ID`) as MIN_ID, max(`ID`) as MAX_ID \n",
    "        FROM `warehouse_stock`.`item_stock`\n",
    "        WHERE `STOCK_DATE` BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND `WAREHOUSE_ID` = '{warehouse_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    min_bounds=0\n",
    "    max_bounds=0\n",
    "    for min_id, max_id in local_cursor:\n",
    "        min_bounds=min_id\n",
    "        max_bounds=max_id\n",
    "    print(\"Query bounds are : \", min_id, max_id)\n",
    "\n",
    "    stock_query=f\"\"\"\n",
    "        SELECT `ID`, date_format(`STOCK_DATE`,'%Y-%m-%d') as STOCK_DATE, `WAREHOUSE_ID`, \n",
    "            `ITEM_NAME`, `OPENING_STOCK`, `RECEIPTS`, `ISSUES`, `UNIT_VALUE`\n",
    "        FROM `warehouse_stock`.`item_stock`\n",
    "        WHERE `STOCK_DATE` BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND `WAREHOUSE_ID` = '{warehouse_id}'\n",
    "    \"\"\"\n",
    "\n",
    "    #Using mysql since there is a bug in mariadb connector for spark\n",
    "    #Using the workaround : https://issues.apache.org/jira/browse/SPARK-25013\n",
    "    stock_df = local_spark.read\\\n",
    "                .format(\"jdbc\")\\\n",
    "                .option(\"url\", \"jdbc:mysql://localhost:3306/warehouse_stock\")\\\n",
    "                .option(\"dbtable\", \"( \" + stock_query + \" ) as tmpStock\")\\\n",
    "                .option(\"user\", \"spark\")\\\n",
    "                .option(\"password\", \"spark\")\\\n",
    "                .option(\"partitionColumn\",\"ID\")\\\n",
    "                .option(\"lowerBound\", min_bounds)\\\n",
    "                .option(\"upperBound\",max_bounds + 1)\\\n",
    "                .option(\"numPartitions\",2)\\\n",
    "                .load()\n",
    "    \n",
    "    stock_df.show(3)\n",
    "\n",
    "    #Save the records to the distributed file system in the Central data center\n",
    "    #Records are partitioned by stock_date and warehouse ID\n",
    "    stock_df.write\\\n",
    "        .mode(\"append\")\\\n",
    "        .partitionBy(\"STOCK_DATE\",\"WAREHOUSE_ID\")\\\n",
    "        .parquet(\"raw_data/\")\n",
    "    \n",
    "start_date = generate_dates[0]\n",
    "end_date = generate_dates[2]\n",
    "\n",
    "#Generate for 3 warehouses\n",
    "upload_stock(start_date,end_date,\"NewYork\")\n",
    "upload_stock(start_date,end_date,\"LosAngeles\")\n",
    "upload_stock(start_date,end_date,\"London\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6d03a-1386-4bd5-a5ce-3b629751637a",
   "metadata": {},
   "source": [
    "### 03.05. Aggregating stock across warehouses\n",
    "\n",
    "Aggregate total stock by item across warehouses and save them to a central mysql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0368fd82-39d4-4f88-ae79-ac9d6fe97313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to global_stock database\n",
    "global_db_conn = mariadb.connect(\n",
    "                user=\"spark\",\n",
    "                password=\"spark\",\n",
    "                host=\"127.0.0.1\",\n",
    "                port=3306,\n",
    "                database=\"global_stock\",\n",
    "                autocommit=True\n",
    "            )\n",
    "\n",
    "global_cursor = global_db_conn.cursor()\n",
    "\n",
    "#create spark session for aggregating\n",
    "global_spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"GlobalAggregatorJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
    "            .config(\"spark.default.parallelism\", 2)\\\n",
    "            .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)\\\n",
    "            .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\",\"2\")\\\n",
    "            .config(\"spark.jars\", \"jars/mysql-connector-j-8.4.0.jar\") \\\n",
    "            .config(\"spark.driver.extraClassPath\",\"jars/mysql-connector-j-8.4.0.jar\") \\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "#Set logging level to warn\n",
    "global_spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "138e4232-fcad-43f2-a3b9-4ffc603e9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete existing data if needed. Execute code if needed to reinitialize database\n",
    "global_cursor.execute(\"DELETE FROM global_stock.item_stock\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0876617-70cd-476e-8505-3707b3ac5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Stock Summary\n",
      "+----------+----------------+---------+-------------+--------+------+-------------+-------------+\n",
      "|STOCK_DATE|       ITEM_NAME|TOTAL_REC|OPENING_STOCK|RECEIPTS|ISSUES|CLOSING_STOCK|CLOSING_VALUE|\n",
      "+----------+----------------+---------+-------------+--------+------+-------------+-------------+\n",
      "|2025-12-31|  Tape Dispenser|        3|          216|      65|   134|          147|       880.53|\n",
      "|2025-12-31|Pencil Sharpener|        3|           66|      40|    48|           58|       580.00|\n",
      "|2025-12-31|Labeling Machine|        3|          153|      63|   155|           61|      1525.00|\n",
      "|2025-12-31|        Scissors|        3|          173|      68|   162|           79|       631.21|\n",
      "|2025-12-31|    Sticky Notes|        3|          136|     101|   175|           62|       124.00|\n",
      "+----------+----------------+---------+-------------+--------+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the global stock data from the central file system\n",
    "#this code reads files for all the dates. The can also be done one date at a time\n",
    "#by appending the date directory to the URL\n",
    "global_stock_df = global_spark.read.\\\n",
    "                    parquet(\"raw_data/\")\n",
    "\n",
    "#Create a view for using SparkSQL\n",
    "global_stock_df.createOrReplaceTempView(\"GLOBAL_STOCK\")\n",
    "\n",
    "#Create a summary dataframe based on a summary query\n",
    "summary_df = global_spark.sql(\"\"\"\n",
    "                    SELECT STOCK_DATE, ITEM_NAME,\n",
    "                            COUNT(*) as TOTAL_REC,\n",
    "                            SUM(OPENING_STOCK) as OPENING_STOCK, \n",
    "                            SUM(RECEIPTS) as RECEIPTS, \n",
    "                            SUM(ISSUES) as ISSUES,\n",
    "                            SUM( OPENING_STOCK + RECEIPTS - ISSUES) as CLOSING_STOCK,\n",
    "                            SUM( (OPENING_STOCK + RECEIPTS - ISSUES) * UNIT_VALUE ) as CLOSING_VALUE \n",
    "                            FROM GLOBAL_STOCK \n",
    "                            GROUP BY STOCK_DATE, ITEM_NAME\n",
    "                        \"\"\")\n",
    "print(\"Global Stock Summary\")\n",
    "summary_df.show(5)\n",
    "\n",
    "#Write usmmary to MariaDB table\n",
    "summary_df.write\\\n",
    "    .mode(\"append\")\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/global_stock\")\\\n",
    "    .option(\"dbtable\", \"global_stock.item_stock\")\\\n",
    "    .option(\"user\", \"spark\")\\\n",
    "    .option(\"password\", \"spark\")\\\n",
    "    .save()\n",
    "\n",
    "#Pruning: After processsing is over, delete the files in raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94670854-f376-4459-b2d5-91fddc78b16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in local databases : \n",
      "------------------------------------\n",
      "NewYork 61 17 41\n",
      "LosAngeles 10 38 47\n",
      "London 47 48 89\n",
      "\n",
      "Data in global database : \n",
      "------------------------------------\n",
      "118 103 177\n"
     ]
    }
   ],
   "source": [
    "#Check if summary is computed correctly\n",
    "#Query local database for individual records\n",
    "local_check_cursor=local_db_conn.cursor()\n",
    "local_check_cursor.execute(\n",
    "    f\"\"\"SELECT `WAREHOUSE_ID`, `OPENING_STOCK`,`RECEIPTS`,`ISSUES`\n",
    "        FROM `warehouse_stock`.`item_stock`\n",
    "        WHERE `ITEM_NAME` = 'Pencil Box' \n",
    "            AND STOCK_DATE = '{start_date}'\n",
    "        \"\"\" )\n",
    "\n",
    "print(\"Data in local databases : \\n------------------------------------\")\n",
    "for warehouse_id, opening_stock, receipts, issues in local_check_cursor:\n",
    "    print(warehouse_id, opening_stock, receipts, issues)\n",
    "\n",
    "#Query global database for summary records\n",
    "global_check_cursor=global_db_conn.cursor()\n",
    "global_check_cursor.execute(\n",
    "    f\"\"\"SELECT `OPENING_STOCK`,`RECEIPTS`,`ISSUES`\n",
    "        FROM `global_stock`.`item_stock`\n",
    "        WHERE `ITEM_NAME` = 'Pencil Box' \n",
    "            AND STOCK_DATE = '{start_date}'\n",
    "        \"\"\" )\n",
    "\n",
    "print(\"\\nData in global database : \\n------------------------------------\")\n",
    "for opening_stock, receipts, issues in global_check_cursor:\n",
    "    print( opening_stock, receipts, issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e559b7-f116-4195-bfec-04bde623c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
