{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5454dd81-2dae-4a7d-aed0-7756acbcac5c",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec248f35-9b8e-4976-960b-b5a9d54023ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.1 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (from pyspark==3.5.1) (0.10.9.7)\n",
      "Requirement already satisfied: findspark==2.0.1 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: pandas==2.2.2 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (from pandas==2.2.2) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (from pandas==2.2.2) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "Requirement already satisfied: mariadb==1.1.10 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (1.1.10)\n",
      "Requirement already satisfied: packaging in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (from mariadb==1.1.10) (25.0)\n",
      "Collecting kafka-python==2.0.2\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: six in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (1.17.0)\n",
      "Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "Requirement already satisfied: redis==5.0.7 in f:\\data engineering\\exercise files\\pyspark\\lib\\site-packages (5.0.7)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Install mariadb-connector-C before proceeding here.\n",
    "\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"3\"\n",
    "\n",
    "!pip install pyspark==3.5.1\n",
    "!pip install findspark==2.0.1\n",
    "!pip install pandas==2.2.2\n",
    "!pip install mariadb==1.1.10\n",
    "!pip install kafka-python>=2.0.3\n",
    "!pip install redis==5.0.7\n",
    "\n",
    "#In\n",
    "#Set the following environment variables\n",
    "# PYSPARK_PYTHON=\"python\"\n",
    "\n",
    "#JAVA_HOME to Java 17\n",
    "#Add JAVA_HOME to PATH\n",
    "\n",
    "#For Windows only\n",
    "#  1. Copy the hadoop folder under \"exercise-files\" to c:\\hadoop\n",
    "#  2. Add an environment variable HADOOP_HOME=c:\\hadoop\n",
    "#  3. Add c:\\hadoop\\bin to PATH\n",
    "#  4. Close the notebook and the command prompt. Reopen command prompt, restart notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2f841-4546-4215-ac56-809f94c0f100",
   "metadata": {},
   "source": [
    "### Setup MariaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cb4fe2-6121-4fd0-8a7d-eb1b2ef47b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases available : ['global_stock', 'information_schema', 'mysql', 'performance_schema', 'spark_de', 'sys', 'warehouse_stock', 'website_stats']\n",
      "warehouse_stock DB already exists.\n",
      "global_stock DB already exists.\n",
      "website_stats DB already exists.\n",
      "\n",
      "Schema and Tables:\n",
      "-----------------------------\n",
      "website_stats  :  visit_stats\n",
      "warehouse_stock  :  item_stock\n",
      "global_stock  :  item_stock\n"
     ]
    }
   ],
   "source": [
    "import mariadb\n",
    "\n",
    "#Connect to mariadb as root user\n",
    "root_conn = mariadb.connect(\n",
    "                user=\"root\",\n",
    "                password=\"spark\",\n",
    "                host=\"127.0.0.1\",\n",
    "                port=3306,\n",
    "                database=\"mysql\"\n",
    "            )\n",
    "\n",
    "db_cursor = root_conn.cursor()\n",
    "db_cursor.execute(\"SHOW DATABASES\")\n",
    "db_databases = db_cursor.fetchall()\n",
    "\n",
    "db_list=[]\n",
    "for database in db_databases:\n",
    "    db_list.append(database[0])\n",
    "    \n",
    "print(\"Databases available :\", db_list)\n",
    "\n",
    "#Create warehouse_stock database & item_stock table\n",
    "if ( \"warehouse_stock\" in db_list):\n",
    "    print(\"warehouse_stock DB already exists.\")\n",
    "else:\n",
    "    print(\"Going to create warehouse_stock DB\")\n",
    "    #Create DB\n",
    "    db_cursor.execute(\"CREATE DATABASE warehouse_stock\")\n",
    "    #Create table\n",
    "    db_cursor.execute(\"\"\"\n",
    "        CREATE TABLE `warehouse_stock`.`item_stock` (\n",
    "               `ID` INT NULL AUTO_INCREMENT,\n",
    "               `STOCK_DATE` DATETIME NOT NULL, \n",
    "               `WAREHOUSE_ID` VARCHAR(45) NOT NULL,\n",
    "               `ITEM_NAME` VARCHAR(45) NOT NULL,\n",
    "               `OPENING_STOCK` INT NOT NULL DEFAULT 0,\n",
    "               `RECEIPTS` INT  NOT NULL DEFAULT 0,\n",
    "               `ISSUES` INT  NOT NULL DEFAULT 0,\n",
    "               `UNIT_VALUE` DECIMAL(10,2)  NOT NULL DEFAULT 0,\n",
    "               PRIMARY KEY (`ID`),\n",
    "               INDEX `STOCK_DATE` (`STOCK_DATE` ASC));\"\"\")\n",
    "    \n",
    "    #Grant privileges to user spark\n",
    "    db_cursor.execute(\"GRANT ALL PRIVILEGES ON warehouse_stock.* to 'spark'@'%'\")\n",
    "    db_cursor.execute(\"FLUSH PRIVILEGES\")\n",
    "    print(\"warehouse_stock DB successfully created\")\n",
    "\n",
    "#Create global_stock database & item_stock table\n",
    "if ( \"global_stock\" in db_list):\n",
    "    print(\"global_stock DB already exists.\")\n",
    "else:\n",
    "    print(\"Going to create global_stock DB\")\n",
    "    #Create DB\n",
    "    db_cursor.execute(\"CREATE DATABASE global_stock\")\n",
    "    #Create table\n",
    "    db_cursor.execute(\"\"\"\n",
    "        CREATE TABLE `global_stock`.`item_stock` (\n",
    "                `ID` INT NULL AUTO_INCREMENT,\n",
    "                `STOCK_DATE` DATETIME NOT NULL,\n",
    "                `ITEM_NAME` VARCHAR(45) NOT NULL,\n",
    "                `TOTAL_REC` INT NOT NULL DEFAULT 0,\n",
    "                `OPENING_STOCK` INT NOT NULL DEFAULT 0,\n",
    "                `RECEIPTS` INT  NOT NULL DEFAULT 0,\n",
    "                `ISSUES` INT  NOT NULL DEFAULT 0,\n",
    "                `CLOSING_STOCK` INT NOT NULL DEFAULT 0,\n",
    "                `CLOSING_VALUE` DECIMAL(10,2)  NOT NULL DEFAULT 0,\n",
    "                PRIMARY KEY (`ID`),\n",
    "                INDEX `STOCK_DATE` (`STOCK_DATE` ASC));\"\"\")\n",
    "    \n",
    "    #Grant privileges to user spark\n",
    "    db_cursor.execute(\"GRANT ALL PRIVILEGES ON global_stock.* to 'spark'@'%'\")\n",
    "    db_cursor.execute(\"FLUSH PRIVILEGES\")\n",
    "    print(\"globak_stock DB successfully created\")\n",
    "\n",
    "#Create website_stats database & visit_stats table\n",
    "if ( \"website_stats\" in db_list):\n",
    "    print(\"website_stats DB already exists.\")\n",
    "else:\n",
    "    print(\"Going to create website_stats DB\")\n",
    "    #Create DB\n",
    "    db_cursor.execute(\"CREATE DATABASE website_stats\")\n",
    "    #Create table\n",
    "    db_cursor.execute(\"\"\"\n",
    "       CREATE TABLE `website_stats`.`visit_stats` (\n",
    "                `ID` int(11) NOT NULL AUTO_INCREMENT, \n",
    "                `INTERVAL_TIMESTAMP` DATETIME DEFAULT NULL,\n",
    "                `LAST_ACTION` varchar(45) DEFAULT NULL,\n",
    "                `DURATION` int(10) DEFAULT NULL, \n",
    "                PRIMARY KEY (`ID`));\"\"\")\n",
    "    \n",
    "    #Grant privileges to user spark\n",
    "    db_cursor.execute(\"GRANT ALL PRIVILEGES ON website_stats.* to 'spark'@'%'\")\n",
    "    db_cursor.execute(\"FLUSH PRIVILEGES\")\n",
    "    print(\"website_stats DB successfully created\")\n",
    "\n",
    "\n",
    "#Check if tables are created\n",
    "db_cursor.execute(\"\"\"\n",
    "    SELECT TABLE_SCHEMA, TABLE_NAME\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema IN ('warehouse_stock','global_stock','website_stats')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSchema and Tables:\\n-----------------------------\")\n",
    "for schema, table in db_cursor:\n",
    "    print(schema,\" : \", table)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623511ad-c3e6-4827-b949-690b8335a959",
   "metadata": {},
   "source": [
    "### Setup Kafka Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8254c901-cead-47ad-ac81-65f8deee21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics available now :\n",
      "--------------------------------\n",
      "spark.streaming.carts.abandoned\n",
      "spark.streaming.website.visits\n",
      "spark.exercise.lastaction.long\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer=KafkaConsumer(\n",
    "                group_id=\"setup\",\n",
    "                bootstrap_servers=\"localhost:9092\" )\n",
    "\n",
    "if \"spark.streaming.website.visits\" not in consumer.topics():\n",
    "\n",
    "    admin_client = KafkaAdminClient(\n",
    "                    bootstrap_servers=\"localhost:9092\", \n",
    "                    client_id='spark-de')\n",
    "    \n",
    "    topic_list = [\n",
    "        NewTopic(name=\"spark.streaming.website.visits\", \n",
    "                 num_partitions=1, replication_factor=1),\n",
    "        NewTopic(name=\"spark.streaming.carts.abandoned\", \n",
    "                 num_partitions=1, replication_factor=1),\n",
    "        NewTopic(name=\"spark.exercise.lastaction.long\", \n",
    "                 num_partitions=1, replication_factor=1)\n",
    "    ]\n",
    "\n",
    "    print(\"Creating topics...\")\n",
    "    result=admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
    "\n",
    "print(\"Topics available now :\\n--------------------------------\")\n",
    "for topic in consumer.topics():\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edeb0b-17b1-470e-a215-bf0dd733a559",
   "metadata": {},
   "source": [
    "### Create a raw data folder to represent a distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33befba-98c6-4fb7-8a39-06002ffcf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./raw_data\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ceabd50-2599-4f29-bc60-3204b404c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\n",
      "C:\\hadoop\n",
      "C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\\bin;C:\\hadoop\\bin;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd\\;C:\\Program Files\\Git\\bin\\;C:\\Program Files\\PuTTY\\;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Program Files\\nodejs\\;c:\\Users\\afiniti\\AppData\\Local\\Programs\\cursor\\resources\\app\\bin;C:\\Program Files\\MariaDB\\MariaDB Connector C 64-bit\\lib\\;C:\\Program Files\\MariaDB\\MariaDB Connector C 64-bit\\lib\\plugin\\;C:\\Users\\afiniti\\.cargo\\bin;C:\\Users\\afiniti\\AppData\\Local\\Programs\\Python\\Python312\\Scripts\\;C:\\Users\\afiniti\\AppData\\Local\\Programs\\Python\\Python312\\;C:\\Users\\afiniti\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\afiniti\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\afiniti\\AppData\\Roaming\\npm;C:\\Program Files\\JetBrains\\WebStorm 2025.1.3\\bin;;C:\\Users\\afiniti\\AppData\\Local\\JetBrains\\Toolbox\\scripts\n"
     ]
    }
   ],
   "source": [
    "#Check if JAVA_HOME is set to Java 1.17\n",
    "print(os.environ[\"JAVA_HOME\"])\n",
    "\n",
    "#Check if HADOOP_HOME is set, needed for windows only\n",
    "print(os.environ[\"HADOOP_HOME\"])\n",
    "\n",
    "#Check if JAVA_HOME & HADOOP_HOME (windows only) are in the PATH\n",
    "print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da776ca-374f-4a29-95d2-764bce61f493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
