{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262d595b-25da-4661-92f4-322f913918bf",
   "metadata": {},
   "source": [
    "### Create a streaming analytics job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea033b-bf55-4dda-9850-d470ff423329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "print(\"Kernel Python:\", sys.executable)\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "print(\"Set PYSPARK_PYTHON:\", os.environ[\"PYSPARK_PYTHON\"])\n",
    "print(\"Set PYSPARK_DRIVER_PYTHON:\", os.environ[\"PYSPARK_DRIVER_PYTHON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2c3a5-4102-4f04-9ff4-ac9c87877fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import mariadb\n",
    "\n",
    "#For production systems, use a class instead\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.foreach.html\n",
    "def write_to_redis(row):\n",
    "    stats_key=\"last-action-stats\"\n",
    "    redis_conn=redis.Redis(host=\"localhost\", \n",
    "                     port=6379, decode_responses=True)\n",
    "    redis_conn.zincrby(stats_key,\n",
    "                        row[\"duration\"],row[\"country\"])\n",
    "    redis_conn.quit()       \n",
    "\n",
    "def write_to_mariadb(row):\n",
    "    #Connect to website_stats database\n",
    "    summary_conn = mariadb.connect(\n",
    "                user=\"spark\",\n",
    "                password=\"spark\",\n",
    "                host=\"127.0.0.1\",\n",
    "                port=3306,\n",
    "                database=\"website_stats\",\n",
    "                autocommit=True\n",
    "            )\n",
    "    summary_cursor = summary_conn.cursor()\n",
    "    \n",
    "    summary_sql=f\"\"\"\n",
    "            INSERT INTO `website_stats`.`visit_stats` \n",
    "                (INTERVAL_TIMESTAMP, LAST_ACTION, DURATION)\n",
    "            VALUES('{row[\"window\"][\"start\"]}',\n",
    "                    '{row[\"last_action\"]}',\n",
    "                    '{row[\"duration\"]}')\n",
    "            \"\"\"\n",
    "    summary_cursor.execute(summary_sql)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129d532-e988-43f7-88e8-6726d4ae0b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Streaming Website Analytics (Corrected)\n",
    "# - Fix visit_date parsing (string -> timestamp)\n",
    "# - Replace per-row foreach sinks with foreachBatch (stable + production-like)\n",
    "# - Add checkpointLocation for ALL streaming queries\n",
    "# - Keep your Windows + venv Python stability configs\n",
    "# =========================\n",
    "\n",
    "import os, sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"*************Starting Streaming Analytics for Website visits*****************\")\n",
    "\n",
    "# ---- IMPORTANT: your producer sends visit_date as a STRING like \"YYYY-MM-DD HH:MM:SS\"\n",
    "# So parse it explicitly in Spark. Declare visit_date as StringType here.\n",
    "schema = StructType([\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"last_action\", StringType()),\n",
    "    StructField(\"visit_date\", StringType()),   # was TimestampNTZType() -> change to StringType()\n",
    "    StructField(\"duration\", IntegerType())\n",
    "])\n",
    "\n",
    "# ---- Spark session (Windows + venv safe)\n",
    "streaming_spark = (SparkSession.builder\n",
    "    .appName(\"StreamingWebsiteAnalyticsJob\")\n",
    "    .master(\"local[2]\")\n",
    "\n",
    "    # Force IPv4 loopback (Windows)\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "\n",
    "    # Force same python as Jupyter kernel\n",
    "    .config(\"spark.pyspark.python\", sys.executable)\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable)\n",
    "\n",
    "    # Windows stability for Python workers\n",
    "    .config(\"spark.python.use.daemon\", \"false\")\n",
    "    .config(\"spark.python.worker.reuse\", \"false\")\n",
    "\n",
    "    # your existing configs\n",
    "    .config(\"spark.sql.shuffle.partitions\", 2)\n",
    "    .config(\"spark.default.parallelism\", 2)\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "    # jars\n",
    "    .config(\"spark.jars\",\n",
    "            \"jars/mysql-connector-j-8.4.0.jar,\"\n",
    "            \"jars/commons-pool2-2.12.0.jar,\"\n",
    "            \"jars/kafka-clients-3.6.0.jar,\"\n",
    "            \"jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,\"\n",
    "            \"jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,\"\n",
    "            \"jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"jars/*\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "streaming_spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark pyspark python:\", streaming_spark.sparkContext.getConf().get(\"spark.pyspark.python\"))\n",
    "print(\"Spark driver python:\", streaming_spark.sparkContext.getConf().get(\"spark.pyspark.driver.python\"))\n",
    "\n",
    "# -------------------------\n",
    "# Read from Kafka\n",
    "# -------------------------\n",
    "print(\"Reading from Kafka...\")\n",
    "raw_visits_df = (streaming_spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"spark.streaming.website.visits\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON + convert visit_date string -> timestamp\n",
    "visits_df = (raw_visits_df\n",
    "    .selectExpr(\"CAST(value AS STRING) as value\")\n",
    "    .select(F.from_json(\"value\", schema).alias(\"visits\"))\n",
    "    .select(\"visits.*\")\n",
    "    .withColumn(\"visit_date\", F.to_timestamp(\"visit_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .filter(F.col(\"visit_date\").isNotNull())   # drop bad records if parse fails\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Write abandoned shopping carts to Kafka (keep as-is, but add value as bytes/string)\n",
    "# -------------------------\n",
    "shopping_cart_df = visits_df.filter(F.col(\"last_action\") == \"ShoppingCart\")\n",
    "\n",
    "q_carts = (shopping_cart_df\n",
    "    .selectExpr('CAST(format_string(\"%s,%s,%s,%d\", country, last_action, visit_date, duration) AS STRING) as value')\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"checkpointLocation\", \"tmp/cp-shoppingcart2\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"spark.streaming.carts.abandoned\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Redis sink (use foreachBatch, not per-row foreach)\n",
    "# -------------------------\n",
    "def write_redis_batch(batch_df, batch_id: int):\n",
    "    import redis\n",
    "    r = redis.Redis(host=\"localhost\", port=6379, decode_responses=True)\n",
    "    stats_key = \"last-action-stats\"\n",
    "\n",
    "    # small batches: collect is OK for learning; in prod you'd do partition-wise writes\n",
    "    rows = batch_df.collect()\n",
    "    if rows:\n",
    "        pipe = r.pipeline()\n",
    "        for row in rows:\n",
    "            pipe.zincrby(stats_key, int(row[\"duration\"]), row[\"country\"])\n",
    "        pipe.execute()\n",
    "    r.close()\n",
    "\n",
    "q_redis = (visits_df\n",
    "    .select(\"country\", \"duration\")\n",
    "    .writeStream\n",
    "    .foreachBatch(write_redis_batch)\n",
    "    .option(\"checkpointLocation\", \"tmp/cp-redis\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Windowed summary -> MariaDB (use foreachBatch)\n",
    "# -------------------------\n",
    "windowed_summary = (visits_df\n",
    "    # Use event time from the record rather than current_timestamp()\n",
    "    .withWatermark(\"visit_date\", \"10 seconds\")\n",
    "    .groupBy(\n",
    "        F.window(F.col(\"visit_date\"), \"5 seconds\"),\n",
    "        F.col(\"last_action\")\n",
    "    )\n",
    "    .agg(F.sum(F.col(\"duration\")).alias(\"duration\"))\n",
    ")\n",
    "\n",
    "def write_mariadb_batch(batch_df, batch_id: int):\n",
    "    import mariadb\n",
    "    conn = mariadb.connect(\n",
    "        user=\"spark\",\n",
    "        password=\"spark\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=3306,\n",
    "        database=\"website_stats\",\n",
    "        autocommit=True\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    rows = batch_df.collect()\n",
    "    data = [(r[\"window\"][\"start\"], r[\"last_action\"], int(r[\"duration\"])) for r in rows]\n",
    "    if data:\n",
    "        cur.executemany(\n",
    "            \"INSERT INTO website_stats.visit_stats (INTERVAL_TIMESTAMP, LAST_ACTION, DURATION) VALUES (?, ?, ?)\",\n",
    "            data\n",
    "        )\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "q_db = (windowed_summary\n",
    "    .writeStream\n",
    "    .foreachBatch(write_mariadb_batch)\n",
    "    .option(\"checkpointLocation\", \"tmp/cp-mariadb\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Await one query (others keep running)\n",
    "q_db.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7cd47-e238-4834-9ac5-ec2ab8f60b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
